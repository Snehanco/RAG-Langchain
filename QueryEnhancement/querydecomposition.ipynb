{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc39471",
   "metadata": {},
   "source": [
    "# Query Enhancement -  Query Decomposition Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363fc3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\RAG-Langchain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c0a4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"langchain_crewai.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size= 500,\n",
    "    chunk_overlap = 50,\n",
    "    separators = [\"\\n\\n\", \"\\n\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7eafc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619bab2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001EC85644E90>, search_type='mmr', search_kwargs={'k': 4, 'lambda_mult': 0.7})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type =\"mmr\",\n",
    "    search_kwargs= {\"k\":4, \"lambda_mult\":0.7}\n",
    ")\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309c4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= init_chat_model(\"groq:openai/gpt-oss-20b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7124336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_prompt = PromptTemplate.from_template(\n",
    "                                                    \"\"\"\n",
    "                                                    You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval. Return the answer with only the sub-questions seperated by new-line characters\n",
    "                                                                                                        \n",
    "                                                    Questions: \"{question}\"\n",
    "                                                                                                        \n",
    "                                                    Sub-questions:\n",
    "                                                    \"\"\")\n",
    "\n",
    "decomposition_chain= decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5c624c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What memory features does LangChain provide?  \\nWhat agent capabilities does LangChain support?  \\nWhat memory and agent implementations does CrewAI use?  \\nHow do LangChain's memory and agent implementations compare to those of CrewAI?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How does Langchain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question = decomposition_chain.invoke({\"question\":query})\n",
    "\n",
    "decomposition_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c87818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                                         Use the context below to answer the question.\n",
    "\n",
    "                                         Context:\n",
    "                                         {context}\n",
    "\n",
    "                                         Question: {input}\n",
    "                                         \"\"\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt= qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b6ad88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_decomposition_rag_pipeline(user_query: str)-> str:\n",
    "    sub_queries_text= decomposition_chain.invoke({\"question\":user_query})\n",
    "    sub_queries=[q.strip() for q in sub_queries_text.split(\"\\n\")]\n",
    "    answers= []\n",
    "    for q in sub_queries:\n",
    "        docs = retriever.invoke(q)\n",
    "        result = qa_chain.invoke({\"input\": q, \"context\": docs})\n",
    "        answers.append(f\"Q: {q}\\nA: {result}\")\n",
    "\n",
    "    return \"\\n\\n\".join(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb1f59e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: \n",
      "\n",
      "Q: What mechanisms does Langchain provide for memory management in its agents?\n",
      "A: **LangChain’s memory‑management tools for agents**\n",
      "\n",
      "| Feature | What it does | Typical use‑case in an agent |\n",
      "|--------|--------------|-----------------------------|\n",
      "| **Memory interfaces** | Abstract base classes (`BaseMemory`) that define the contract for storing and retrieving data. | Lets any agent plug in a custom memory backend without changing its logic. |\n",
      "| **Built‑in memory implementations** | • **ConversationBufferMemory** – keeps a simple buffer of the last N turns.<br>• **ConversationSummaryMemory** – keeps a running summary of the conversation instead of the full history.<br>• **ConversationTokenBufferMemory** – stores history up to a token budget.<br>• **ConversationEntityMemory** – stores structured entities extracted from the dialogue.<br>• **ConversationBufferWindowMemory** – keeps a sliding window of the most recent turns. | Agents can choose the most appropriate strategy: a full buffer for short chats, a summary for long‑running sessions, or an entity store for knowledge‑driven tasks. |\n",
      "| **Memory adapters** | Small helper classes that wrap a memory implementation and expose a clean `save_context` / `load_context` API. | Allows agents to automatically persist context after each step. |\n",
      "| **Integration with Chains** | Chains can be wrapped with a `Memory` object so that each step’s input is enriched with the stored context. | Ensures that every LLM call in a chain sees the same conversational history. |\n",
      "| **Persistence options** | Memory objects can be backed by in‑memory data structures, Redis, SQLite, or any custom persistence layer via the `BaseMemory` interface. | Agents can survive restarts or be shared across multiple users. |\n",
      "| **Retrieval‑augmented memory** | Combine a `VectorStore` (e.g., FAISS, Pinecone) with a memory module to fetch relevant documents or prior Q&A snippets on demand. | Agents can pull in external knowledge or past support tickets automatically. |\n",
      "\n",
      "### How agents use it\n",
      "\n",
      "1. **Instantiate a memory object** (e.g., `ConversationBufferMemory`) and pass it to the agent constructor.  \n",
      "2. **During each turn**, the agent’s `run` method calls `memory.save_context()` to record the user’s input and the agent’s response.  \n",
      "3. **When generating a reply**, the agent calls `memory.load_context()` to retrieve the conversation history or summary, which is then injected into the prompt template.  \n",
      "4. **Optional persistence** ensures the memory survives beyond the current process (e.g., by serializing to a database or Redis).  \n",
      "\n",
      "In short, LangChain gives agents a plug‑and‑play set of memory modules—buffers, summaries, token‑bounded stores, entity maps, and vector‑retrieval adapters—so that they can maintain context, remember past interactions, and even persist that knowledge across sessions.\n",
      "\n",
      "Q: How are agents implemented in Langchain, and what are their key characteristics?\n",
      "A: **How LangChain implements agents**\n",
      "\n",
      "| Layer | What it is | How it’s built in LangChain |\n",
      "|-------|------------|-----------------------------|\n",
      "| **LLM chain** | The “brain” that receives the current state, the list of available tools, and the user’s goal, and outputs a *plan* or a *tool‑call* | `LLMChain` (or `ChatPromptTemplate` + `ChatOpenAI`) with a prompt that includes the tool list and a “think‑then‑act” instruction. |\n",
      "| **Tool abstraction** | A callable that performs a specific external action (DB query, API request, summarization, etc.) | `Tool` objects – each has a name, description, and a callable function. |\n",
      "| **Agent executor** | The control loop that repeatedly feeds the LLM’s output back to the chain, calls the chosen tool, and aggregates the results | `AgentExecutor` (or `AgentExecutor.from_llm_and_tools`) orchestrates the loop. |\n",
      "| **Memory** | Keeps the conversation history, tool outputs, and the agent’s own reasoning so it can maintain context over many turns | `ConversationBufferMemory`, `ConversationSummaryMemory`, or any custom `BaseMemory` implementation. |\n",
      "| **Prompt templates** | Standardized prompt that tells the LLM what it can do and how to format its answer | `PromptTemplate` or `ChatPromptTemplate` that includes placeholders for tools, memory, and the user’s request. |\n",
      "| **Custom agents** | If the default “zero‑shot” or “few‑shot” strategy isn’t enough, you can subclass `BaseAgent` or `ZeroShotAgent` and override `plan()` or `construct_prompt()` | `CustomAgent = type('CustomAgent', (ZeroShotAgent,), {...})` or a full subclass. |\n",
      "\n",
      "**Key characteristics of LangChain agents**\n",
      "\n",
      "| Characteristic | Why it matters |\n",
      "|----------------|----------------|\n",
      "| **Dynamic action selection** | The agent can decide, at runtime, which tool to use (query, API, summarizer, etc.) based on the current context. |\n",
      "| **Tool integration** | Any external system—databases, APIs, custom scripts—can be wrapped as a `Tool` and plugged into the agent. |\n",
      "| **Modular design** | You can pick and choose the LLM, memory type, prompt style, and tool set; the agent logic stays the same. |\n",
      "| **Chainable** | An agent can be the final step in a larger chain or can itself invoke other agents, enabling nested workflows. |\n",
      "| **Memory‑aware** | By feeding past tool outputs and LLM responses back into the prompt, the agent stays coherent over long conversations or multi‑step tasks. |\n",
      "| **Reason‑then‑act** | The prompt typically forces the LLM to first produce a plan or an “action” before executing it, which improves reliability. |\n",
      "| **Zero‑shot / Few‑shot flexibility** | You can use a purely zero‑shot prompt or seed it with a few example tool calls to guide the LLM’s behavior. |\n",
      "| **Extensibility** | New tool types, new LLMs, or new decision logic can be added without touching the core loop. |\n",
      "\n",
      "**Bottom line**\n",
      "\n",
      "LangChain implements agents as a *LLM‑driven decision engine* that sits inside an `AgentExecutor`. The LLM chain, together with a prompt template that lists available `Tool` objects, decides the next action. The executor calls the chosen tool, feeds the result back into the chain, and repeats until the goal is satisfied. The whole setup is memory‑aware, modular, and can be extended or customized to fit any workflow—from simple question‑answering to complex multi‑step pipelines like CrewAI.\n",
      "\n",
      "Q: In what ways does CrewAI handle memory differently from Langchain?\n",
      "A: **CrewAI vs. LangChain: How Memory is Handled Differently**\n",
      "\n",
      "| Feature | LangChain | CrewAI |\n",
      "|---------|-----------|--------|\n",
      "| **Purpose of Memory** | Preserve *context* for a single chain or conversation. | Preserve *task state* and *team knowledge* across a multi‑agent workflow. |\n",
      "| **Scope** | Usually **per‑chain** (or per‑conversation). Each chain has its own memory store. | **Shared across the crew**. All agents can read/write the same memory pool. |\n",
      "| **Typical Memory Types** | • `ConversationBufferMemory` – stores raw dialogue<br>• `ConversationSummaryMemory` – stores a compressed summary<br>• `VectorStoreRetriever` – for long‑term retrieval | • **Task Memory** – keeps track of who is doing what, deadlines, and intermediate results.<br>• **Shared Knowledge Base** – a vector store or database of findings, decisions, and insights that every crew member can query.<br>• **Crew‑Wide Log** – a running log of actions, approvals, and changes that all agents can inspect. |\n",
      "| **Persistence** | Can be *in‑memory* or persisted to a file/DB, but usually tied to a single run. | Persisted by design (often to a database or vector store) so that the crew can pick up where they left off even after restarts. |\n",
      "| **Access Pattern** | A single agent (or LLM) reads its own memory, appends new tokens, and passes it back to the chain. | Multiple agents read/write the same memory. When an agent finishes a task, it writes results; another agent can later read those results to decide next steps. |\n",
      "| **Granularity** | Fine‑grained, token‑level context for language generation. | Coarse‑grained, task‑level or document‑level information (e.g., “Paper #12 summary”, “Customer #45 issue type”). |\n",
      "| **Integration with Workflow** | Memory is usually just part of the prompt. It does not drive task assignment or scheduling. | Memory is the backbone of the workflow: it informs task assignment, triggers follow‑ups, and drives automation. |\n",
      "| **Example Use‑Case** | *“Remember the last 10 user queries to keep context.”* | *“Store the status of all support tickets so that any agent can pick up the next open ticket.”* |\n",
      "\n",
      "### Bottom Line\n",
      "- **LangChain** focuses on **contextual memory** for natural‑language generation within a single chain or conversation.  \n",
      "- **CrewAI** focuses on **collaborative, task‑oriented memory** that is shared across agents, persistent, and tightly coupled to workflow automation.  \n",
      "\n",
      "Thus, while both frameworks use memory, LangChain’s memory is about *keeping the conversation coherent*, whereas CrewAI’s memory is about *keeping the crew coordinated and the workflow on track*.\n",
      "\n",
      "Q: How does CrewAI implement agents, and how does this differ from Langchain's agent design?\n",
      "A: **CrewAI’s Agent Implementation**\n",
      "\n",
      "| Feature | CrewAI | LangChain |\n",
      "|---------|--------|-----------|\n",
      "| **Scope & Purpose** | Agents are built to *operate within a human‑team workflow*. Each agent has a clear role (e.g., “Researcher”, “Writer”, “QA”). The system’s core goal is to assign tasks, orchestrate collaboration, and automate follow‑up actions. | Agents are designed to *solve a single user query* by selecting and calling tools. The focus is on a one‑off problem‑solving loop. |\n",
      "| **Architecture** | • **Crew** – a collection of agents + a **Crew Manager** that distributes tasks, merges outputs, and escalates to humans when needed.<br>• **Memory & Context** – shared team memory (e.g., a project board) that all crew members read/write, enabling cumulative knowledge.<br>• **Human‑in‑the‑Loop** – agents can hand off to or request input from actual people, and the platform tracks approvals, comments, and status. | • **Single Agent** – a single LLM instance that decides which tool to call next based on the current prompt.<br>• **Tool‑Calling Pattern** – the agent uses a prompt template that lists available tools; it selects one, calls it, and then continues or ends.<br>• **No Built‑in Workflow Engine** – each agent run is stateless except for the LLM’s internal context. |\n",
      "| **Task Assignment** | The Crew Manager evaluates the user query, breaks it into subtasks, and assigns each to the most suitable agent. It also monitors progress, re‑assigns if an agent stalls, and aggregates results for the final deliverable. | The agent decides on a single tool to use and executes it. If the tool fails or returns a partial answer, the agent can loop, but there’s no external scheduler. |\n",
      "| **Automation** | Automates routine follow‑ups (emails, Slack messages, ticket updates) through dedicated agents that run on a schedule or in response to state changes. | Automation is possible but must be coded into the chain; the agent itself does not manage background jobs. |\n",
      "| **Collaboration** | Built‑in support for team chat, shared dashboards, and role‑based permissions. Agents can “talk” to each other via shared memory and the manager orchestrates the conversation. | No built‑in team collaboration; the agent’s output is typically a single response or a list of tool calls. |\n",
      "\n",
      "**Key Differences**\n",
      "\n",
      "1. **Multi‑Agent vs. Single‑Agent**  \n",
      "   - *CrewAI* treats agents as a **crew** that work together, each with a specialty, under a manager.  \n",
      "   - *LangChain* usually works with a **single agent** that selects tools on its own.\n",
      "\n",
      "2. **Workflow Orchestration**  \n",
      "   - CrewAI has a **task scheduler** that assigns subtasks, tracks progress, and escalates to humans.  \n",
      "   - LangChain’s agent runs in a linear, stateless loop; any workflow control must be coded manually.\n",
      "\n",
      "3. **Human Integration**  \n",
      "   - CrewAI’s agents are designed for *team collaboration*; they can hand off to humans and record approvals.  \n",
      "   - LangChain’s agents are typically *fully autonomous* (unless you embed a human‑in‑the‑loop prompt yourself).\n",
      "\n",
      "4. **Shared Memory & Knowledge**  \n",
      "   - CrewAI provides a **shared project memory** (e.g., a knowledge base or task board) that all agents can read/write.  \n",
      "   - LangChain agents rely on the LLM’s prompt context; persistent memory must be managed externally.\n",
      "\n",
      "5. **Automation & Notifications**  \n",
      "   - CrewAI includes built‑in agents for sending follow‑up emails, Slack messages, or updating CRM records.  \n",
      "   - In LangChain, you’d create separate chains or external scripts to handle such automation.\n",
      "\n",
      "In short, **CrewAI implements agents as part of a coordinated, team‑centric workflow engine** that handles task distribution, progress monitoring, and human collaboration. **LangChain’s agent design is a lightweight, LLM‑driven tool‑caller** focused on solving individual queries without built‑in orchestration or team collaboration features.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does Langchain use memory and agents compared to CrewAI?\"\n",
    "final_answer = query_decomposition_rag_pipeline(query)\n",
    "print(\"Final Answer: \\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
