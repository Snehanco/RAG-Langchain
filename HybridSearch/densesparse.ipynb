{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8092d9d6",
   "metadata": {},
   "source": [
    "# Hybrid Retriever - Combing Dense and Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319c015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\RAG-Langchain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebf641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"Langchain helps build LLM applications\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for Semantic Search\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris\"),\n",
    "    Document(page_content=\"Langchain can be used to develop Agentic AI application\"),\n",
    "    Document(page_content=\"Langchain has many tyoe of retrievers\")\n",
    "]\n",
    "\n",
    "# Step 2: Dense retriever (FAISS + HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-miniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e0fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Spare retriever (BM25)\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3  ## top k docs to retriever\n",
    "\n",
    "## Step 4 : Combine with Ensemble Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weights=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a3ee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000243DA10F7D0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x00000243DEDED8D0>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61686e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: \n",
      " Langchain helps build LLM applications\n",
      "Doc 2: \n",
      " Langchain can be used to develop Agentic AI application\n",
      "Doc 3: \n",
      " Langchain has many tyoe of retrievers\n",
      "Doc 4: \n",
      " Pinecone is a vector database for Semantic Search\n"
     ]
    }
   ],
   "source": [
    "## Step 5 : Query and get results\n",
    "query =  \"How can I build applications using LLMs?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step 6 -  Print\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Doc {i+1}: \\n {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5c171",
   "metadata": {},
   "source": [
    "# RAG pipeline with Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1bf0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e96e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "                                      Answer the question strictly based on the context below. Do not add any information pther than what is present in the context\n",
    "                                      \n",
    "                                      Context:\n",
    "                                      {context}\n",
    "\n",
    "                                      Question:\n",
    "                                      {input}\n",
    "                                      \"\"\")\n",
    "\n",
    "llm= init_chat_model(\"groq:openai/gpt-oss-20b\", temperature = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85d6768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000243DA10F7D0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x00000243DEDED8D0>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n                                      Answer the question strictly based on the context below. Do not add any information pther than what is present in the context\\n\\n                                      Context:\\n                                      {context}\\n\\n                                      Question:\\n                                      {input}\\n                                      ')\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000243DF7C8290>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000243DFA10D90>, model_name='openai/gpt-oss-20b', temperature=1.0, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Stuff Document Chain\n",
    "document_chain= create_stuff_documents_chain(llm=llm, prompt =prompt)\n",
    "\n",
    "\n",
    "## Create Full Rag Chain\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever, combine_docs_chain= document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ee85f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " You can build an app that uses LLMs by following these steps:\n",
      "\n",
      "1. **Use Langchain** – It is a framework that helps you create LLM‑based applications.\n",
      "2. **Add agentic capability** – Langchain can be used to develop agentic AI applications, letting the LLM act autonomously or in a workflow.\n",
      "3. **Incorporate retrievers** – Langchain provides various types of retrievers to fetch relevant information for the LLM.\n",
      "4. **Store and search vectors** – Use Pinecone, a vector database, for semantic search to retrieve the most relevant documents or data for the LLM.\n",
      "\n",
      " Source documents:\n",
      "Doc 1: \n",
      " Langchain helps build LLM applications\n",
      "Doc 2: \n",
      " Langchain can be used to develop Agentic AI application\n",
      "Doc 3: \n",
      " Langchain has many tyoe of retrievers\n",
      "Doc 4: \n",
      " Pinecone is a vector database for Semantic Search\n"
     ]
    }
   ],
   "source": [
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response =  rag_chain.invoke(query)\n",
    "\n",
    "print(\"Answer:\\n\", response[\"answer\"])\n",
    "\n",
    "print (\"\\n Source documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"Doc {i+1}: \\n {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706d9c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse k: 3\n",
      "dense k: None\n",
      "dense results: 4\n",
      "sparse results: 3\n",
      "hybrid results: 4\n",
      "rag response context len: 4\n"
     ]
    }
   ],
   "source": [
    "# Debug: see how many docs each retriever returns\n",
    "print(\"sparse k:\", getattr(sparse_retriever, \"k\", None))\n",
    "print(\"dense k:\", getattr(dense_retriever, \"k\", None))\n",
    "\n",
    "q = \"How can I build applications using LLMs?\"\n",
    "print(\"dense results:\", len(dense_retriever.invoke(q)))\n",
    "print(\"sparse results:\", len(sparse_retriever.invoke(q)))\n",
    "print(\"hybrid results:\", len(hybrid_retriever.invoke(q)))\n",
    "\n",
    "# Check the rag_chain output\n",
    "resp = rag_chain.invoke({\"input\": q})\n",
    "print(\"rag response context len:\", len(resp.get(\"context\", [])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
