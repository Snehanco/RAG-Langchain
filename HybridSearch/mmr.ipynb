{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbf12d7",
   "metadata": {},
   "source": [
    "# Maximal Marginal Relevance technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84ca746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\RAG-Langchain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0562d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain.txt'}, page_content='# LangChain Modules and Use Cases\\n\\nLangChain provides support for several main modules, each designed to address specific aspects of building applications with language models. For each module, LangChain offers examples to get started, how-to guides, reference documentation, and conceptual overviews. Below is a detailed breakdown of these modules, organized in increasing order of complexity.\\n\\n---\\n\\n## 1. Prompts'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 1. Prompts\\n\\nThe **Prompts** module focuses on prompt design, management, and optimization. It includes tools and utilities for:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Prompt Templates**: Construct templates that accept variables, few-shot exemplars, and structured prompts for different LLM APIs.\\n- **Utilities**: Support for prompt serialization, versioning, A/B testing, and guarding against prompt injection.\\n- **Best Practices**: Modular prompts, prompt unit tests, and automated prompt tuning workflows.\\n- **Real-World Examples**: Demonstrations of prompt scaffolding for classification, generation, translation, and role-based responses.\\n\\n---\\n\\n## 2. LLMs'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 2. LLMs\\n\\nThe **LLMs** module provides a consistent interface for interacting with various language model providers and flavors (e.g., text models, chat models, streaming outputs). Key features include:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Configuration Options**: Temperature/top-p, max tokens, streaming, and callback handlers.\\n- **Concurrency Controls**: Request caching and rate-limit backoff strategies.\\n- **Utilities**: Batching, prompt truncation, sampling diagnostics, and multi-model orchestration for ensemble or fallback strategies.\\n- **Usage Guidance**: Model selection, cost/performance tradeoffs, and safety filters.\\n\\n---\\n\\n## 3. Document Loaders'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 3. Document Loaders\\n\\nThe **Document Loaders** module standardizes the ingestion of data from diverse sources, including:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Supported Sources**: Local files (txt, md), PDFs, Word docs, HTML pages, email archives, cloud storage (S3, GCS, Azure), databases, and third-party connectors.\\n- **Features**: Parsing, text extraction, metadata preservation, OCR integration for scanned documents, chunking strategies, and encoding normalization.\\n- **Robustness**: Retry logic, provenance metadata, deduplication, and preprocessing hooks for cleaning, sanitization, and language detection.\\n\\n---\\n\\n## 4. Utils'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 4. Utils\\n\\nThe **Utils** module provides a broad collection of utilities to make LLM applications practical and composable. Examples include:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Code Execution**: Python REPL/tool execution for code generation workflows.\\n- **Embedding Helpers**: Tools for embedding generation and text normalization.\\n- **Knowledge Utilities**: Calculators, date/time parsers, and connectors to external services.\\n- **Other Utilities**: Logging, telemetry hooks, prompt caching, retry policies, and adapters for search engines and vector databases.\\n\\n---\\n\\n## 5. Chains'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 5. Chains\\n\\nThe **Chains** module models multi-step workflows that combine prompts, LLM calls, and utilities into reusable pipelines. Features include:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Flow Types**: Sequential chains, branching/conditional logic, parallel executions, and nested composition.\\n- **Error Handling**: Input/output schemas, instrumentation for latency and token usage, and tooling for unit testing and mocking.\\n- **Common Patterns**: Question-answering over documents, summarization pipelines, classification pipelines, and transform-then-generate flows.\\n\\n---\\n\\n## 6. Indexes'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 6. Indexes\\n\\nThe **Indexes** module focuses on organizing and querying large text corpora to augment LLM responses. It includes:\\n\\n- **Index Types**: Dense vector indexes (FAISS, HNSW, Annoy), sparse/inverted indexes, and hybrid strategies.\\n- **Utilities**: Index construction, incremental updates, sharding, persistence, and reindexing.\\n- **Retrieval Strategies**: k-NN, reranking, MMR, and techniques for long-document handling and chunk-level scoring.\\n\\n---\\n\\n## 7. Agents'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 7. Agents\\n\\nThe **Agents** module implements decision-making loops where an LLM chooses actions, executes tools, receives observations, and iterates until a termination condition is met. Features include:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Agent Frameworks**: Tool adapters (APIs, crawlers, calculators, databases), action schemas, and planner/executor patterns.\\n- **Safety Features**: Action whitelisting, step limits, sandboxing, and observation sanitization.\\n- **Examples**: Web search assistants, API-driven automation, task orchestration, and multi-tool reasoning flows.\\n\\n---\\n\\n## 8. Memory\\n\\nThe **Memory** module provides mechanisms for persisting and recalling conversational or task state across sessions. Key features include:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Implementations**: Ephemeral in-memory buffers, key-value stores, vectorized memories, and summary-based long-term memory.\\n- **Capabilities**: Memory retrieval strategies, staleness management, privacy-preserving storage, and configurable retention policies.\\n- **Patterns**: Summarizing long histories, selectively surfacing relevant facts, and combining memory with retrieval-augmented generation.\\n\\n---\\n\\n## 9. Chat'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 9. Chat\\n\\nThe **Chat** module focuses on message-oriented models and conversation protocols. It includes:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Message Handling**: Formalized message roles (system, user, assistant), conversation threading, partial/streamed message handling, and multi-turn context windows.\\n- **Integrations**: UI adapters, platform connectors (Slack, Teams, Webchat), role-based directives, and moderation/safety hooks.\\n- **Utilities**: Stitching chat transcripts into prompts, summarizing conversation context, and handling multi-user or multi-agent dialogues.\\n\\n---\\n\\n## 10. Evaluation & Monitoring'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 10. Evaluation & Monitoring\\n\\nThe **Evaluation & Monitoring** module provides tools for evaluating model outputs and monitoring application health. Features include:'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='- **Evaluation Pipelines**: Automated evaluation (human-in-the-loop, LM-based scoring, rubric-driven grading).\\n- **Testing**: Unit and integration test harnesses for chains/agents.\\n- **Metrics**: Quality, latency, cost, token usage, error rates, hallucination detection heuristics, and drift alerts.\\n- **Best Practices**: Continuous evaluation, synthetic test cases for regressions, and dashboards for observability and model benchmarking.\\n\\n---\\n\\n# Use Cases'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='---\\n\\n# Use Cases\\n\\nLangChain modules can be used in a variety of ways. Below are some of the common use cases supported by LangChain:\\n\\n### 1. Agents\\nAgents use a language model to interact with other tools. They can be used for grounded question/answering, interacting with APIs, or taking actions.\\n\\n### 2. Chatbots\\nLanguage models are ideal for creating chatbots due to their ability to produce coherent and contextually relevant text.'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='### 3. Data Augmented Generation\\nThis involves chains that interact with external data sources to fetch data for use in the generation step. Examples include summarization of long texts and question/answering over specific data sources.\\n\\n### 4. Question Answering\\nAnswering questions over specific documents, utilizing only the information in those documents to construct an answer.\\n\\n### 5. Summarization\\nSummarizing longer documents into shorter, more condensed chunks of information.'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='### 6. Evaluation\\nGenerative models are hard to evaluate with traditional metrics. LangChain provides prompts and chains to assist in evaluation using language models themselves.\\n\\n### 7. Generating Similar Examples\\nGenerating similar examples to a given input is a common use case for many applications. LangChain provides tools to assist with this.'),\n",
       " Document(metadata={'source': 'langchain.txt'}, page_content='### 8. Comparing Models\\nExperimenting with different prompts, models, and chains is a key part of developing the best possible application. The ModelLaboratory makes this process easier.\\n\\n---\\n\\nLangChain provides a comprehensive suite of tools and modules to simplify and accelerate the development of applications powered by language models. By leveraging these modules, developers can build robust, scalable, and efficient solutions tailored to their specific needs.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"langchain.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size= 500,\n",
    "    chunk_overlap = 50,\n",
    "    separators = [\"\\n\\n\", \"\\n\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99ae779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(docs, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f9d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MMR retriever\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type= \"mmr\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720a9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Anser the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm= init_chat_model(\"groq:openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e2de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Pipeline\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc954e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer :\n",
      " **LangChain’s support for agents and memory**\n",
      "\n",
      "| Feature | How LangChain implements it |\n",
      "|---------|-----------------------------|\n",
      "| **Agents** | • The **Agents** module implements a *decision‑making loop*: an LLM selects an action, a tool is executed, the tool’s observation is fed back to the LLM, and the cycle repeats until a termination condition is reached. <br>• Agents can use any tool that LangChain exposes (HTTP calls, database queries, custom code, etc.). <br>• The framework ships with pre‑built agent templates (e.g., *Zero‑Shot ReAct*, *Tool‑Calling*, *Planner‑Executor*) and the ability to plug in custom logic. |\n",
      "| **Memory for Chatbots** | • **Ephemeral in‑memory buffers** for short‑term context. <br>• **Key‑value stores** (e.g., Redis, SQLite) to persist chat state across sessions. <br>• **Vector‑based memories** that embed past turns and allow similarity‑search retrieval. <br>• **Summary‑based long‑term memory** that condenses a long conversation into a concise summary that can be recalled later. <br>• Built‑in **retrieval‑augmented generation** patterns that combine the LLM’s generation with retrieved facts from memory. |\n",
      "| **Memory‑related capabilities** | • **Staleness management** – automatically flag or refresh stale facts. <br>• **Privacy‑preserving storage** – encryption and access‑control options for sensitive data. <br>• **Configurable retention policies** – keep only the most recent N turns, or keep a summary after a certain length. <br>• **Selective surfacing** – surface only the most relevant facts to the LLM to keep the prompt short. |\n",
      "\n",
      "In short, LangChain gives you a ready‑made agent framework that can call arbitrary tools, and a flexible memory subsystem that can be as simple as an in‑memory buffer or as sophisticated as vector‑retrieval with summarization, all while handling privacy, staleness, and retention.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"How does Langchain support agents and memory?\"})\n",
    "\n",
    "print(\"Answer :\\n\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef9fd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does Langchain support agents and memory?',\n",
       " 'context': [Document(id='c1963cdd-8812-4b85-812c-5d6cd7f55e61', metadata={'source': 'langchain.txt'}, page_content='---\\n\\n# Use Cases\\n\\nLangChain modules can be used in a variety of ways. Below are some of the common use cases supported by LangChain:\\n\\n### 1. Agents\\nAgents use a language model to interact with other tools. They can be used for grounded question/answering, interacting with APIs, or taking actions.\\n\\n### 2. Chatbots\\nLanguage models are ideal for creating chatbots due to their ability to produce coherent and contextually relevant text.'),\n",
       "  Document(id='baf13151-3855-49a4-bc33-07e5c22ade8a', metadata={'source': 'langchain.txt'}, page_content='- **Implementations**: Ephemeral in-memory buffers, key-value stores, vectorized memories, and summary-based long-term memory.\\n- **Capabilities**: Memory retrieval strategies, staleness management, privacy-preserving storage, and configurable retention policies.\\n- **Patterns**: Summarizing long histories, selectively surfacing relevant facts, and combining memory with retrieval-augmented generation.\\n\\n---\\n\\n## 9. Chat'),\n",
       "  Document(id='ba743df3-4a57-4859-a365-bc80adae4d22', metadata={'source': 'langchain.txt'}, page_content='---\\n\\n## 7. Agents\\n\\nThe **Agents** module implements decision-making loops where an LLM chooses actions, executes tools, receives observations, and iterates until a termination condition is met. Features include:')],\n",
       " 'answer': '**LangChain’s support for agents and memory**\\n\\n| Feature | How LangChain implements it |\\n|---------|-----------------------------|\\n| **Agents** | • The **Agents** module implements a *decision‑making loop*: an LLM selects an action, a tool is executed, the tool’s observation is fed back to the LLM, and the cycle repeats until a termination condition is reached. <br>• Agents can use any tool that LangChain exposes (HTTP calls, database queries, custom code, etc.). <br>• The framework ships with pre‑built agent templates (e.g., *Zero‑Shot ReAct*, *Tool‑Calling*, *Planner‑Executor*) and the ability to plug in custom logic. |\\n| **Memory for Chatbots** | • **Ephemeral in‑memory buffers** for short‑term context. <br>• **Key‑value stores** (e.g., Redis, SQLite) to persist chat state across sessions. <br>• **Vector‑based memories** that embed past turns and allow similarity‑search retrieval. <br>• **Summary‑based long‑term memory** that condenses a long conversation into a concise summary that can be recalled later. <br>• Built‑in **retrieval‑augmented generation** patterns that combine the LLM’s generation with retrieved facts from memory. |\\n| **Memory‑related capabilities** | • **Staleness management** – automatically flag or refresh stale facts. <br>• **Privacy‑preserving storage** – encryption and access‑control options for sensitive data. <br>• **Configurable retention policies** – keep only the most recent N turns, or keep a summary after a certain length. <br>• **Selective surfacing** – surface only the most relevant facts to the LLM to keep the prompt short. |\\n\\nIn short, LangChain gives you a ready‑made agent framework that can call arbitrary tools, and a flexible memory subsystem that can be as simple as an in‑memory buffer or as sophisticated as vector‑retrieval with summarization, all while handling privacy, staleness, and retention.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
