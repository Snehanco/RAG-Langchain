{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2216e6",
   "metadata": {},
   "source": [
    "# Introduction to Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bae5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e5c1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup done.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter)\n",
    "\n",
    "print(\"Setup done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d7f81",
   "metadata": {},
   "source": [
    "# Understanding Document Structure in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ed9fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "Content: This is the main text content that will be embedded and searched\n",
      "Metadata: {'source': 'sample.txt', 'author': 'John Doe', 'page': 1, 'date_created': '2024-06-15', 'length': 67, 'keywords': ['sample', 'document', 'langchain']}\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "## create sample document\n",
    "doc= Document(page_content=\"This is the main text content that will be embedded and searched\", metadata={\"source\": \"sample.txt\",\n",
    "                                                                                                         \"author\": \"John Doe\",\n",
    "                                                                                                         \"page\": 1,\n",
    "                                                                                                         \"date_created\": \"2024-06-15\",\n",
    "                                                                                                         \"length\": 67,\n",
    "                                                                                                         \"keywords\": [\"sample\", \"document\", \"langchain\"]\n",
    "                                                                                                         })\n",
    "\n",
    "print(\"Document Structure\")\n",
    "\n",
    "print(f\"Content: {doc.page_content}\")\n",
    "print(f\"Metadata: {doc.metadata}\")\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25733dad",
   "metadata": {},
   "source": [
    "# Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb70f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a text file\n",
    "import os\n",
    "os.makedirs(\"data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4178dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created.\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"data/text_files/sample1.txt\": \"\"\"This is the content of sample text file one. It contains some example text for testing.\n",
    "\n",
    "    Machine Learning with text in Python involves applying various algorithms and techniques to extract insights, classify, or generate text. This process generally follows a structured pipeline:\n",
    "Text Preprocessing:\n",
    "Lowercasing/Uppercasing: Standardizing case to treat words like \"The\" and \"the\" as the same.\n",
    "Eliminating Stopwords: Removing common words (e.g., \"a\", \"the\", \"is\") that often carry little semantic meaning.\n",
    "Stemming/Lemmatization: Reducing words to their root form (e.g., \"running\", \"ran\" to \"run\") to handle variations.\n",
    "Removing Punctuation and Numbers: Cleaning the text of non-alphabetic characters.\n",
    "Feature Vectorization:\n",
    "Bag-of-Words (BoW): Representing text as a collection of word counts, ignoring grammar and word order.\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): Weighing words based on their frequency in a document and rarity across the entire corpus.\n",
    "Word Embeddings (e.g., Word2Vec, GloVe): Representing words as dense vectors in a continuous vector space, capturing semantic relationships.\n",
    "Building and Training ML Models:\n",
    "Supervised Learning: Training models for tasks like text classification (e.g., sentiment analysis, spam detection), where labeled data is available. Common algorithms include Naive Bayes, Support Vector Machines (SVMs), and deep learning models like Recurrent Neural Networks (RNNs) and Transformers.\n",
    "Unsupervised Learning: Applying models for tasks like topic modeling (e.g., Latent Dirichlet Allocation - LDA) or clustering, where data is unlabeled.\n",
    "Model Evaluation:\n",
    "Assessing the model's performance using metrics relevant to the task (e.g., accuracy, precision, recall, F1-score for classification).\n",
    "Key Python Libraries:\n",
    "NLTK (Natural Language Toolkit): For basic text processing, tokenization, stemming, and lemmatization.\n",
    "spaCy: For advanced NLP tasks, including named entity recognition, dependency parsing, and part-of-speech tagging.\n",
    "scikit-learn: For implementing various machine learning algorithms and feature extraction techniques (e.g., CountVectorizer, TfidfVectorizer).\n",
    "pandas: For data manipulation and handling text data in DataFrames.\n",
    "TensorFlow/PyTorch: For building and training deep learning models for more complex text tasks.\n",
    "\"\"\",\n",
    "    \"data/text_files/sample2.txt\": \"\"\"This is the content of sample text file two. It provides additional example text for testing.\n",
    "\n",
    "    Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and human language. The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is valuable. NLP encompasses a variety of tasks, including:\n",
    "Tokenization: Breaking down text into smaller units, such as words or sentences.\n",
    "Part-of-Speech Tagging: Identifying the grammatical parts of speech (nouns, verbs, adjectives, etc.) in a sentence.\n",
    "Named Entity Recognition (NER): Detecting and classifying named entities (people, organizations, locations, etc.) in text.\n",
    "Sentiment Analysis: Determining the sentiment or emotional tone of a piece of text (positive, negative, neutral).\n",
    "Machine Translation: Automatically translating text from one language to another.\"\"\"\n",
    "}\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f3a57",
   "metadata": {},
   "source": [
    "# Text Loader - Read single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0bd4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 Documents\n",
      "<class 'list'>\n",
      "Content preview: This is the content of sample text file one. It contains some example text for testing.\n",
      "\n",
      "    Machine\n",
      "Metadata: {'source': 'data/text_files/sample1.txt'}\n",
      "[Document(metadata={'source': 'data/text_files/sample1.txt'}, page_content='This is the content of sample text file one. It contains some example text for testing.\\n\\n    Machine Learning with text in Python involves applying various algorithms and techniques to extract insights, classify, or generate text. This process generally follows a structured pipeline:\\nText Preprocessing:\\nLowercasing/Uppercasing: Standardizing case to treat words like \"The\" and \"the\" as the same.\\nEliminating Stopwords: Removing common words (e.g., \"a\", \"the\", \"is\") that often carry little semantic meaning.\\nStemming/Lemmatization: Reducing words to their root form (e.g., \"running\", \"ran\" to \"run\") to handle variations.\\nRemoving Punctuation and Numbers: Cleaning the text of non-alphabetic characters.\\nFeature Vectorization:\\nBag-of-Words (BoW): Representing text as a collection of word counts, ignoring grammar and word order.\\nTF-IDF (Term Frequency-Inverse Document Frequency): Weighing words based on their frequency in a document and rarity across the entire corpus.\\nWord Embeddings (e.g., Word2Vec, GloVe): Representing words as dense vectors in a continuous vector space, capturing semantic relationships.\\nBuilding and Training ML Models:\\nSupervised Learning: Training models for tasks like text classification (e.g., sentiment analysis, spam detection), where labeled data is available. Common algorithms include Naive Bayes, Support Vector Machines (SVMs), and deep learning models like Recurrent Neural Networks (RNNs) and Transformers.\\nUnsupervised Learning: Applying models for tasks like topic modeling (e.g., Latent Dirichlet Allocation - LDA) or clustering, where data is unlabeled.\\nModel Evaluation:\\nAssessing the model\\'s performance using metrics relevant to the task (e.g., accuracy, precision, recall, F1-score for classification).\\nKey Python Libraries:\\nNLTK (Natural Language Toolkit): For basic text processing, tokenization, stemming, and lemmatization.\\nspaCy: For advanced NLP tasks, including named entity recognition, dependency parsing, and part-of-speech tagging.\\nscikit-learn: For implementing various machine learning algorithms and feature extraction techniques (e.g., CountVectorizer, TfidfVectorizer).\\npandas: For data manipulation and handling text data in DataFrames.\\nTensorFlow/PyTorch: For building and training deep learning models for more complex text tasks.\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/text_files/sample1.txt\", encoding=\"utf-8\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} Documents\")\n",
    "print(type(documents))\n",
    "print(f\"Content preview: {documents[0].page_content[:100]}\")  \n",
    "print(f\"Metadata: {documents[0].metadata}\")\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36085a75",
   "metadata": {},
   "source": [
    "# DirectoryLoader - Multiple text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b4f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1990.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 Documents from directory\n",
      "\n",
      "Document 1:\n",
      "Content length: 2301 characters\n",
      "Source: data\\text_files\\sample1.txt\n",
      "\n",
      "Document 2:\n",
      "Content length: 926 characters\n",
      "Source: data\\text_files\\sample2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load all text files from a directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"data/text_files\", \n",
    "    glob=\"**/*.txt\", ## Pattern to match files\n",
    "    loader_cls= TextLoader, ## Loader class to use\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} Documents from directory\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content length: {len(doc.page_content)} characters\")  \n",
    "    print(f\"Source: {doc.metadata['source']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
